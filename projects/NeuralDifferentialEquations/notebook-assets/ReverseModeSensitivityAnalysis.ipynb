{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f07f270",
   "metadata": {},
   "source": [
    "# Reverse Mode Sensitivity Analysis \n",
    "\n",
    "* With the approach from [Chen et al](https://arxiv.org/abs/1806.07366): *Reverse-mode adjoint sensitivity analysis*, we can compute derivates of the trajectories : \n",
    "\n",
    "* Given is an ODE $\\dot{\\mathbf{x}} = f(\\mathbf{x},t;\\theta)$ that is integrated from $t_0$ to $t_1$ with parameters $\\theta$ and a scalar loss function $L(\\mathbf{x}(t_1))$ of the ODE solution that is supposed to be minimized by the training procedure \n",
    "* The parameters $\\theta$ can include those of data-driven models like ANN\n",
    "* To compute the gradient $$\\frac{\\partial\\mathcal{L}}{\\partial\\theta},$$ the ODE is appended with the adjoint\n",
    "$$\\begin{align}\n",
    "    \\mathbf{a}(t) = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}(t)}.\n",
    "\\end{align}$$\n",
    "* The adjoint follows the dynamics\n",
    "$$\\begin{align}\n",
    "    \\frac{d \\mathbf{a}}{d t} = - \\mathbf{a}^T(t) \\frac{\\partial f(\\mathbf{x}(t),t,\\theta)}{\\partial \\mathbf{x}(t)}\n",
    "\\end{align}$$\n",
    "and tracks how the gradient of the loss depends on the trajectory. This is needed to compute the desired $\\frac{\\partial\\mathcal{L}}{\\partial\\theta}$ with another adjoint\n",
    "\\begin{align}\n",
    "    \\mathbf{a}_\\theta &= \\frac{\\partial\\mathcal{L}}{\\partial\\theta(t)} \\\\\n",
    "    \\frac{d \\mathbf{a}_\\theta}{d t}& = - \\mathbf{a}^T(t) \\frac{\\partial f(\\mathbf{x}(t),t,\\theta)}{\\partial \\theta}\n",
    "\\end{align}\n",
    "\n",
    "## Proof\n",
    "The proof of these dynamics can be seen as a continuous backpropagation. Similar to the traditional, discrete backprogation the chain rule is applied with\n",
    "\\begin{align}\n",
    " \\frac{d\\mathcal{L}}{d\\mathbf{x}(t)}&=\\frac{d\\mathcal{L}}{d\\mathbf{x}(t+\\epsilon)}\\frac{d\\mathbf{x}(t+\\epsilon)}{d \\mathbf{x}(t)}\\label{eq:cont-L}\n",
    "\\end{align}\n",
    "by inserting a state of the trajectory evolved by an incremental time step $\\epsilon$. Evolving the trajectory can be approximated with \n",
    "\\begin{align}\n",
    "     \\mathbf{x}(t+\\epsilon) &= \\int_t^{t+\\epsilon} f(\\mathbf{x}(t),t;\\theta)dt + \\mathbf{x}(t) = T_\\epsilon(\\mathbf{x}(t),t;\\theta) \\overset{\\epsilon\\rightarrow 0}{\\approx} \\epsilon f(\\mathbf{x}(t),t;\\theta) + \\mathbf{x}(t)\n",
    "\\end{align}\n",
    "to rewrite the chain rule equation above as \n",
    "\\begin{align}\n",
    "\\mathbf{a}(t)&=\\mathbf{a}(t+\\epsilon)\\frac{\\partial T_\\epsilon(\\mathbf{x}(t),t)}{\\partial \\mathbf{x}(t)}.\n",
    "\\end{align}\n",
    "These results can be used to get the dynamics of $\\mathbf{a}(t)$ by inserting them into the definition of its derivative:  \n",
    "\\begin{align}\n",
    "      \\frac{d\\mathbf{a}(t)}{dt} &= \\lim_{\\epsilon\\rightarrow 0} \\frac{1}{\\epsilon}(\\mathbf{a}(t+\\epsilon) - \\mathbf{a}(t)) \\\\\n",
    "    &= \\lim_{\\epsilon\\rightarrow 0} \\frac{1}{\\epsilon}(\\mathbf{a}(t+\\epsilon) - \\mathbf{a}(t+\\epsilon)\\frac{\\partial}{\\partial\\mathbf{x}(t)}(\\mathbf{x}(t)+\\epsilon f(\\mathbf{x}(t),t;\\theta))\\\\\n",
    "    &= \\lim_{\\epsilon\\rightarrow 0} - \\mathbf{a}(t+\\epsilon)\\frac{\\partial f(\\mathbf{x}(t),t;\\theta)}{\\partial\\mathbf{x}(t)}=- \\mathbf{a}(t) \\frac{\\partial f(\\mathbf{x}(t),t,\\theta)}{\\partial \\mathbf{x}(t)} .\n",
    "\\end{align}\n",
    "The dynamics of $\\mathbf{a}_\\theta$ can be derived analogously. \n",
    "\n",
    "\n",
    "## Reverse-mode Adjoint Sensitivity Problem \n",
    "\n",
    "Similar to how a traditional backpropagation traverses the chain of the ANN from the output back to the input, the appended ODE with the adjoints needs to be solved backwards in time as the initial values of the augmented dynamics are only known at the end point of the integration $t_1$. To compute $\\frac{\\partial\\mathcal{L}}{\\partial\\theta}$, we thus need to solve the appended ODE\n",
    "\\begin{align} \n",
    "    \\begin{pmatrix}\n",
    "        \\frac{d \\mathbf{x}}{d t}\\\\\n",
    "        \\frac{d \\mathbf{a}}{d t}\\\\ \n",
    "        \\frac{d \\mathbf{a}_\\theta}{d t}\\\\\n",
    "    \\end{pmatrix} =\n",
    "     \\begin{pmatrix}\n",
    "        f(\\mathbf{x},t,\\theta)\\\\\n",
    "        - \\mathbf{a}^T(t) \\frac{\\partial f(\\mathbf{x}(t),t,\\theta)}{\\partial \\mathbf{x}(t)} \\\\ \n",
    "        - \\mathbf{a}^T(t) \\frac{\\partial f(\\mathbf{x}(t),t,\\theta)}{\\partial \\theta}\\label{eq:node-train}\n",
    "    \\end{pmatrix} \n",
    "\\end{align}\n",
    "backwards in time from $t_1$ to $t_0$ with initial conditions $[\\mathbf{x}(t_1); \\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{x}(t_1)}; \\mathbf{0}]$ to eventually get $\\frac{\\partial\\mathcal{L}}{\\partial\\theta} = \\mathbf{a}_\\theta(t_0)$. The partial derivatives are computed using AD. \n",
    "\n",
    "## Other Sensitivity Algorithms \n",
    "\n",
    "There are also many other sensitivty algorithms, several of them are implement in `DiffEqSensitivity`. In most cases these are: \n",
    "\n",
    "* `QuadratureAdjoint`, `InterpolatingAdjoint` and `BacksolveAdjoint`: [Kim et al](https://arxiv.org/abs/2103.15341) outline these algorithm, espacially with regards to their properties for stiff differential equation problems\n",
    "* For ergotic, chaotic system, we can also use [least square shadowing methods](https://arxiv.org/abs/1204.0159), with `AdjointLSS` or `NILSAS` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
