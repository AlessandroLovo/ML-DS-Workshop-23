{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9010587b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning for Dynamical Systems "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81ddf0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial Neural Networks: A quick summary\n",
    "\n",
    "Artificial neural networks (ANNs), in the form of multilayer perceptrons (MLPs) are:\n",
    "\n",
    "* Networks made up of a chain of several layers: $$f(\\mathbf x) = f^{(3)}(f^{(2)}(f^{(1)}(\\mathbf x; \\theta^{(1)}); \\theta^{(2)}); \\theta^{(3)}),$$ where each layers is a parametrized nonlinear transformation: $$\\sigma(\\mathbf z^{\\mathrm T} \\mathbf W + \\mathbf b),$$ where $\\theta=\\{\\mathbf{W},\\mathbf{b}\\}$ are the learnable parameters and $\\sigma$ is a nonlinear activation function, such as $\\tanh$\n",
    "* They are *universal function approximators* with many parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b101c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Given training data, we seek the best set of parameters by minimzing a loss function $L(\\theta)$, e.g. a mean square error, on that training data, by means of a gradient descent optimization, for which we need $$\\nabla_\\theta L(\\mathbf \\theta).$$ \n",
    "* The gradients are computed via *backpropagation*, i.e. chain rule of derivatives\n",
    "* ML libaries compute this via an automatic differenation system, that is able to systematically track all elementary operations performed\n",
    "* The parameters are then updated with some form of gradient descent \n",
    "$$\\theta_{new} = \\theta - \\eta \\nabla_\\theta L(\\mathbf \\theta)$$, \n",
    "with some learning rate $\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2828019",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Key Ingredients \n",
    "\n",
    "* Training data \n",
    "* A parametrized model \n",
    "* A loss function \n",
    "* The ability to take gradients of the loss function, to update the parameters of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c8841a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML for Dynamical System \n",
    "\n",
    "How can you ML methods like ANN for dynamical sytems? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1667c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### Apprixmate a DS \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d32b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prior Knowledge "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cd621",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Methods for this Workshop \n",
    "\n",
    "* Neural Differential Equations \n",
    "* Reservoir Computing \n",
    "* Symbolic Methods (SINDy) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4939141",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Differential Equations \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527bffd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reservoir Computing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0637df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Symbolic Methods (SINDy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45891996",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Projects\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
