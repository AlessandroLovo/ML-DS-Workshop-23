{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c8841a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning for Dynamical Systems \n",
    "\n",
    "How can we learn dynamical systems from data? \n",
    "How can we use Machine Learning methods like Artificial Neural Networks for dynamical sytems? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cd621",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Methods for this Workshop \n",
    "\n",
    "* Neural Differential Equations \n",
    "* Reservoir Computing \n",
    "* Symbolic Methods (SINDy) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f8863",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Approximate a Dynamical System \n",
    "\n",
    "\n",
    "* Suppose we have a dynamical system $$\\frac{d\\mathbf{x}}{dt} = f(\\mathbf{x},t;\\theta)$$ that we observe some data from $\\mathbf{X}=\\{\\mathbf{x}(t_i)\\}$ for $t_i\\in[t_0;t_f]$, in the beginning we will restrict ourselves to evenly sampled observations $t_i = i\\cdot\\Delta t + t_0$\n",
    "\n",
    "* The correspoding discretized dynamical system is $$\\mathbf{x}_{n+1} = g(\\mathbf{x}_n, t_n; \\theta)$$ where $g$ is one iteration of some numerical DE solver\n",
    "\n",
    "* With **Neural Differential Equations** we replaced $f$ with an ANN and used it as a universal function approximator to learn the right hand side of the dynamical system from the observation $\\mathbf{X}$\n",
    " \n",
    "* With **Recurrent Neural Networks** (RNNs) like **Reservoir Computing**, we try to replace $g$ with ANN an learn it from data \n",
    "\n",
    "* When we want to recover the actual analytical expression of $f$, we have to rely on symbolic methods such as **Sparse Indentification of Dynamical Systems (SINDy)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fdd37",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prior Knowledge \n",
    "\n",
    "* Aside from MLPs, there are many different types of ANNs, e.g:![image-2.png](projects/NeuralDifferentialEquations/notebook-assets/typesofanns.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81ddf0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial Neural Networks: A quick summary\n",
    "\n",
    "Artificial neural networks (ANNs), in the form of multilayer perceptrons (MLPs) are:\n",
    "\n",
    "* Networks made up of a chain of several layers: $$f(\\mathbf x) = f^{(3)}(f^{(2)}(f^{(1)}(\\mathbf x; \\theta^{(1)}); \\theta^{(2)}); \\theta^{(3)}),$$ where each layers is a parametrized nonlinear transformation: $$\\sigma(\\mathbf z^{\\mathrm T} \\mathbf W + \\mathbf b),$$ where $\\theta=\\{\\mathbf{W},\\mathbf{b}\\}$ are the learnable parameters and $\\sigma$ is a nonlinear activation function, such as $\\tanh$\n",
    "* They are *universal function approximators* with many parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b101c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Given training data, we seek the best set of parameters by minimzing a loss function $L(\\theta)$, e.g. a mean square error, on that training data, by means of a gradient descent optimization, for which we need $$\\nabla_\\theta L(\\mathbf \\theta).$$ \n",
    "* The gradients are computed via *backpropagation*, i.e. chain rule of derivatives\n",
    "* ML libaries compute this via an automatic differenation system, that is able to systematically track all elementary operations performed\n",
    "* The parameters are then updated with some form of gradient descent \n",
    "$$\\theta_{new} = \\theta - \\eta \\nabla_\\theta L(\\mathbf \\theta)$$, \n",
    "with some learning rate $\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2828019",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Key Ingredients \n",
    "\n",
    "* Training data \n",
    "* A parametrized model \n",
    "* A loss function \n",
    "* The ability to take gradients of the loss function, to update the parameters of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4939141",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Differential Equations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef470f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Differential Equations \n",
    "\n",
    "* With Neural Differential Equations (Neural DEs) we try to find a way to combine knowledge that we have of systems in form of their governing equations with data-driven approximators such as ANNs\n",
    "\n",
    "* In fact, there is an analogy between differential equations and ANNs:\n",
    "\n",
    "* A Residual Network (ResNet) block is defined by $$\\mathbf{h}_{t+1} = \\mathbf{h}_t + f(\\mathbf{h}_t;\\theta_t),$$ where $f$ can be any combination of other neural network layers with parameters $\\theta_t$\n",
    "\n",
    "* Through their short cut connection (see the image), ResNets learn a residual (hence the name). They proved to be an effective architecture for a wide variety of problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a078b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Compare that to the Euler solver to discretize and solve an ODE: \n",
    "$$\\begin{align}  \n",
    "\\frac{d\\mathbf{h}(t)}{dt} &= f(\\mathbf{h}(t),t;\\theta)\\\\\n",
    "\\mathbf{h}_{t+1} &= \\Delta t f(\\mathbf{h}_t, t;\\theta) + \\mathbf{h}_t\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "* Differential equations can be seen as a continuous time limit of ResNet ANNs\n",
    "* There are several paper that use to just solve ResNets with ODE solvers, but this is not our primary interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bbd92d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Universal Differential Equations Framework\n",
    "\n",
    "* If we can treat differential equations and ANNs so similar, we just combine them directly:\n",
    "\n",
    "$$\\frac{du}{dt} = f(u,t,U_\\theta(u,t)),$$ \n",
    "where $U_{\\theta}$ is some data-driven function approximator (such as an ANN)\n",
    "  \n",
    "![image-2.png](projects/NeuralDifferentialEquations/notebook-assets/overview2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67c26c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We can integrate these Neural Differential Equations numerically, like any other differential equation we've seen before, resulting in a trajectory $$\\hat{\\mathbf{u}}(\\mathbf{x},t;\\theta)$$\n",
    "\n",
    "* `DiffEqFlux.jl`/`DiffEqSensitivity.jl` are by far the most comprehensive implementation of this approach (of all programming languages)\n",
    "* `torchdiffeq` offers some of the functionality for pyTorch\n",
    "* `diffrax` offers some of the functionality for JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a42264",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to train them? \n",
    "\n",
    "* We train them by minimizing a loss function $L(\\theta)$, e.g.: \n",
    "    * Given some example trajectories $\\mathbf{u}$ as training data, we can define a loss function $$L(\\theta)= \\sum_{i_t,\\mathbf{x}} ( \\mathbf{u}(\\mathbf{x},i_t) - \\hat{\\mathbf{u}}(\\mathbf{x},i_t;\\theta) )^2$$\n",
    "\n",
    "* For this approach to work we need the ability to take the derivatives of the trajectories\n",
    "* We can in fact do this by combining adjoint sensitivity analysis with automatic differentiation techniques (AD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527bffd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reservoir Computing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331b6b0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Reservoir computing is a type of **Recurrent neural networks (RNNs)**,  a family of neural networks for processing sequential data.\n",
    "\n",
    "* Consider a sequence of the form\n",
    "\n",
    "$$\\mathbf s^{(t)} = f(\\mathbf s^{(t-1)}; \\mathbf \\theta),$$\n",
    "\n",
    "where $\\mathbf s^{(t)}$ is a vector describing the value of the sequence at the discrete index $t$, and $\\mathbf \\theta$ are some parameters of the function $f$.\n",
    "\n",
    "![rnn1](notebook-assets/rnn-1.png) [Source](https://www.deeplearningbook.org/)\n",
    "\n",
    "* We call such a sequence **recurrent** because the definition of $\\mathbf s$ at time $t$ refers back to the same definition at time $t-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11259a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* More generally, we can also consider the RNN exhibiting memory (e.g. of an external driver of the dynamical system)\n",
    "\n",
    "* This is usually realised by including a hidden state $\\mathbf{h}$\n",
    "\n",
    "$$\\mathbf h^{(t)} = f(\\mathbf h^{(t-1)}, \\mathbf x^{(t)}; \\mathbf \\theta),$$\n",
    "\n",
    "* We update the hidden state at each iteration, and the output of at each step is usually given as a function of the hidden state:\n",
    "\n",
    "$$\\mathbf{s}^{(t)} = g(\\mathbf{h}^{(t)}, \\mathbf{s}^{(t-1)}; \\theta)$$\n",
    "\n",
    "This will be the **general form of a RNN** that we will consider. \n",
    "\n",
    "* A RNN can develop self-sustained dynamics due to its recurrent connections, even in the absence of input. Indeed, it can be shown that, under fairly mild and general assumptions, that RNNs are **universal approximators of dynamical systems**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f73f78",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* RNNs suffer from the **vanishing and exploding gradient problem**\n",
    "* When computing gradients through a long chain of RNN iterations, the gradients are scaled with the eigenvalues of the iteration operation\n",
    "* This can lead to the gradients vanishing or exploding \n",
    "* There are several ways to mitigate this problem\n",
    "* One particular simple one is **Reservoir Computing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5949274",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reservoir Computing \n",
    "\n",
    "* Reservoir computing is one implemenation of RNNs \n",
    "\n",
    "![rnn1](notebook-assets/reservoir.png)\n",
    "\n",
    "* They are ANNs with \n",
    "    * an input layer $W_{in}$\n",
    "    * a large hidden layer, the reservoir, that is usually a sparse random network $W$\n",
    "    * an output layer $W_{out}$\n",
    "    \n",
    "* The key trick that reservoir computing does is that **only the output layer is trained** and the input layer and the reservoir are randomly initialized but constant \n",
    "\n",
    "* This has the advantage that the training can be done via linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c16e7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Reservoir computing has been both applied successfully to prototypical chaotic systems, and climate phenomena \n",
    "* See e.g:\n",
    "    1. [Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data (Pathak et al. 2017)](https://aip.scitation.org/doi/10.1063/1.5010300)\n",
    "    2. [Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach (Pathak et al. 2018)](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.024102)\n",
    "    3. [Seasonal prediction of Indian summer monsoon onset with echo state networks (Mitsui & Boers 2021)](https://iopscience.iop.org/article/10.1088/1748-9326/ac0acb)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0637df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Symbolic Methods (SINDy)\n",
    "\n",
    "* Another approach to estimate the dynamical system is to try to directly estimate it's equation from data, so that we really to reconstruct the symbolic expression of $f$ and not just numerically approximate the $f$ with a function approximator such as an ANN\n",
    "\n",
    "* **Symbolic Regression** \n",
    "\n",
    "    * But instead of already prescriping the functional form in a concrete way, can't we also let an algorithm find the functional form? \n",
    "    * Symbolic Regression tries to find the mathematical expression that best fits the $\\mathbf{X}$\n",
    "    * Applied to dynamical systems, it usually tries to find the mathematical expression for the right hand side $f$ of the system\n",
    "    * Therefore we often also need derivative data (e.g. computed with finite differences) \n",
    "    * Symbolic regression usually provides a dictionary of possible expressions (e.g. polynomials up to a certain degree, trigonemtric functions, etc ...) and than performs a regression to dermine the coefficents or parameters of these elementary functions\n",
    "    * But there are infintely many combinations of expressions: \n",
    "    \n",
    "    ![Symbolic Regression](notebook-assets/slice2.jpg)\n",
    "    \n",
    "    * Most natural laws and equations just involve a handful of terms, a candidate model should be complex enough to replicate the behaviour of the system but also \"simple\" (see Occam's razor)\n",
    "    * Therefore often some form of sparsity constraint is applied to the regression and one choses to only consider certain operations and experessions \n",
    "    * [AI Feynmann by Udrescu and Tegmark](https://arxiv.org/abs/1905.11481) attracted some attention: they do a symbolic regression with several different pre- and post-processing steps and apply it successfully to Feynmann's physics course books\n",
    "    \n",
    "\n",
    "* All of these methods have limitations when the complexity of the problem increases, data gets noisy and high-dimensional\n",
    "\n",
    "* One reason: There are just too many possible combinations of expressions to be considered \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb8382",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projects\n",
    "\n",
    "Now it's your turn! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa36f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Projects \n",
    "\n",
    "* For each of the methods we prepared recources for you to get started with them and work on a small project. \n",
    "\n",
    "* You find everything at https://github.com/TUM-PIK-ESM/ML-DS-Workshop-23 \n",
    "\n",
    "* Best, you clone the repository "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886c315",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recources  \n",
    "\n",
    "* A Julia cheat-sheet \n",
    "* [Projects with Neural Differential Equations](projects/NeuralDifferentialEquations/NeuralDifferentialEquations.ipynb)\n",
    "* Reservoir Computing \n",
    "* SINdy"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
