{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c8841a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning for Dynamical Systems \n",
    "\n",
    "How can we learn dynamical systems from data? \n",
    "\n",
    "How can we use Machine Learning methods like Artificial Neural Networks for dynamical sytems? \n",
    "\n",
    "Can we incorporate prior knowledge into these methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cd621",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Methods for this Workshop \n",
    "\n",
    "* Neural Differential Equations \n",
    "* Reservoir Computing \n",
    "* Symbolic Methods (SINDy) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81ddf0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial Neural Networks: A quick summary\n",
    "\n",
    "Artificial neural networks (ANNs), in the form of multilayer perceptrons (MLPs) are:\n",
    "\n",
    "* Networks made up of a chain of several layers: $$f(\\mathbf x) = f^{(3)}(f^{(2)}(f^{(1)}(\\mathbf x; \\theta^{(1)}); \\theta^{(2)}); \\theta^{(3)}),$$ where each layers is a parametrized nonlinear transformation: $$\\sigma(\\mathbf z^{\\mathrm T} \\mathbf W + \\mathbf b),$$ where $\\theta=\\{\\mathbf{W},\\mathbf{b}\\}$ are the learnable parameters and $\\sigma$ is a nonlinear activation function, such as $\\tanh$\n",
    "* They are *universal function approximators* with many parameters $\\theta$ (in program code here often `p`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b101c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Given training data, we seek the best set of parameters by minimzing a loss function $L(\\theta)$, e.g. a mean square error, on that training data, by means of a gradient descent optimization, for which we need $$\\nabla_\\theta L(\\mathbf \\theta).$$ \n",
    "* The gradients are computed via *backpropagation*, i.e. chain rule of derivatives\n",
    "* ML libaries compute this via an automatic differenation system, that is able to systematically track all elementary operations performed\n",
    "* The parameters are then updated with some form of gradient descent \n",
    "$$\\theta_{new} = \\theta - \\eta \\nabla_\\theta L(\\mathbf \\theta)$$, \n",
    "with some learning rate $\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2828019",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Key Ingredients \n",
    "\n",
    "* **Training data**: usually pairs of input and output examples $(x,y)$ of the function we try to approximate\n",
    "* **A parametrized model**: e.g. a MLP \n",
    "* **A loss function**: e.g a least square error $L(\\theta)=\\sum \\left(\\text{MLP}(x;\\theta) - y\\right)^2$\n",
    "* The ability to take **gradients of the loss function**, to update the parameters of the model $\\theta_{new} = \\theta - \\eta \\nabla_\\theta L(\\mathbf \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c175d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So how can we use these approaches for dynamical systems? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f8863",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Approximate a Dynamical System \n",
    "\n",
    "\n",
    "* Suppose we have a dynamical system $$\\frac{d\\mathbf{x}}{dt} = f(\\mathbf{x},t;\\theta)$$ that we observe some data from $\\mathbf{X}=\\{\\mathbf{x}(t_i)\\}$ for $t_i\\in[t_0;t_f]$, in the beginning we will restrict ourselves to evenly sampled observations $t_i = i\\cdot\\Delta t + t_0$\n",
    "\n",
    "* The correspoding discretized dynamical system is $$\\mathbf{x}_{n+1} = g(\\mathbf{x}_n, t_n; \\theta)$$ where $g$ is one iteration of some numerical DE solver\n",
    "\n",
    "* With **Neural Differential Equations** we replaced $f$ with an ANN and use it as a universal function approximator to learn the right hand side of the dynamical system from the observation $\\mathbf{X}$\n",
    " \n",
    "* With **Recurrent Neural Networks** (RNNs) like **Reservoir Computing**, we try to replace $g$ with ANN an learn it from data \n",
    "\n",
    "* When we want to recover the actual analytical expression of $f$, we have to rely on symbolic methods such as **Sparse Indentification of Dynamical Systems (SINDy)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fdd37",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prior Knowledge \n",
    "\n",
    "* Aside from MLPs, there are many different types of ANNs, e.g:![image-2.png](projects/NeuralDifferentialEquations/notebook-assets/typesofanns.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4939141",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Differential Equations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef470f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Differential Equations \n",
    "\n",
    "* With Neural Differential Equations (Neural DEs) we try to find a way to combine knowledge that we have of systems in form of their governing equations with data-driven approximators such as ANNs\n",
    "\n",
    "* In fact, there is an analogy between differential equations and ANNs:\n",
    "\n",
    "* A Residual Network (ResNet) block is defined by $$\\mathbf{h}_{t+1} = \\mathbf{h}_t + f(\\mathbf{h}_t;\\theta_t),$$ where $f$ can be any combination of other neural network layers with parameters $\\theta_t$\n",
    "\n",
    "* Through their short cut connection (see the image), ResNets learn a residual (hence the name). They proved to be an effective architecture for a wide variety of problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca1c42",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Compare that to the Euler solver to discretize and solve an ODE: \n",
    "$$\\begin{align}  \n",
    "\\frac{d\\mathbf{h}(t)}{dt} &= f(\\mathbf{h}(t),t;\\theta)\\\\\n",
    "\\mathbf{h}_{t+1} &= \\Delta t f(\\mathbf{h}_t, t;\\theta) + \\mathbf{h}_t\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "* Differential equations can be seen as a continuous time limit of ResNet ANNs\n",
    "* There are several paper that use to just solve ResNets with ODE solvers, but this is not our primary interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b94c5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Universal Differential Equations Framework\n",
    "\n",
    "* If we can treat differential equations and ANNs so similar, we just combine them directly:\n",
    "\n",
    "$$\\frac{du}{dt} = f(u,t,U_\\theta(u,t)),$$ \n",
    "where $U_{\\theta}$ is some data-driven function approximator (such as an ANN)\n",
    "  \n",
    "![image-2.png](projects/NeuralDifferentialEquations/notebook-assets/overview2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200e035",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We can integrate these Neural Differential Equations numerically, like any other differential equation we've seen before, resulting in a trajectory $$\\hat{\\mathbf{u}}(\\mathbf{x},t;\\theta)$$\n",
    "\n",
    "* `DiffEqFlux.jl`/`DiffEqSensitivity.jl` are by far the most comprehensive implementation of this approach (of all programming languages)\n",
    "* `torchdiffeq` offers some of the functionality for pyTorch\n",
    "* `diffrax` offers some of the functionality for JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4d124",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to train them? \n",
    "\n",
    "* We train them by minimizing a loss function $L(\\theta)$, e.g.: \n",
    "    * Given some example trajectories $\\mathbf{u}$ as training data, we can define a loss function $$L(\\theta)= \\sum_{i_t,\\mathbf{x}} ( \\mathbf{u}(\\mathbf{x},i_t) - \\hat{\\mathbf{u}}(\\mathbf{x},i_t;\\theta) )^2$$\n",
    "\n",
    "* For this approach to work we need the ability to take the derivatives of the trajectories\n",
    "* We can in fact do this by combining adjoint sensitivity analysis with automatic differentiation techniques (AD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc3806",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Adjoint Sensitivity Analysis \n",
    "\n",
    "* We can compute derivates $\\frac{\\partial\\hat{u}(\\mathbf{x},i_t;\\theta)}{\\partial\\theta}$ by the means of adjoint sensitivity analysis and AD\n",
    "* Adjoint sensitivity analysis solves an augmented version of the original DE system, appended with adjoint, e.g. $\\frac{d \\mathbf{a}}{d t} = - \\mathbf{a}^T(t) \\frac{\\partial f(\\mathbf{x}(t),t,\\theta)}{\\partial \\mathbf{x}(t)}$ that helps to compute the wanted derivatives \n",
    "* More details can be found in the [additional material of the Neural DE project](projects/NeuralDifferentialEquations/notebook-assets/ReverseModeSensitivityAnalysis.ipnyb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade5c0a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Are Gradients of Chaotic Systems Meaningful?\n",
    "\n",
    "* Nearby trajectories of chaotic systems diverge exponentially with their Lyapunov exponents\n",
    "* Can we still compute meaningful ways are: \n",
    "\n",
    "* Two possible ways:  \n",
    "    * Least square shadowing (for ergodic system and very long trajectories) \n",
    "    * \"Multiple shooting\": Start with short trajectories, slowly increase the length \n",
    "$$L(\\theta)= \\sum_{i_t}^{\\tau} \\sum_\\mathbf{x} ( \\mathbf{u}(\\mathbf{x},i_t) - \\hat{\\mathbf{u}}(\\mathbf{x},i_t;\\theta) )^2$$\n",
    "    * Start with $\\tau$ small, increase it during training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91ff8ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example Applications \n",
    "\n",
    "![CGLE](notebook-assets/cgle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527bffd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reservoir Computing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f04c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Reservoir computing is a type of **Recurrent neural networks (RNNs)**,  a family of neural networks for processing sequential data.\n",
    "\n",
    "* Consider a sequence of the form\n",
    "\n",
    "$$\\mathbf s^{(t)} = f(\\mathbf s^{(t-1)}; \\mathbf \\theta),$$\n",
    "\n",
    "where $\\mathbf s^{(t)}$ is a vector describing the value of the sequence at the discrete index $t$, and $\\mathbf \\theta$ are some parameters of the function $f$.\n",
    "\n",
    "![rnn1](notebook-assets/rnn-1.png) [Source](https://www.deeplearningbook.org/)\n",
    "\n",
    "* We call such a sequence **recurrent** because the definition of $\\mathbf s$ at time $t$ refers back to the same definition at time $t-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add838c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* More generally, we can also consider the RNN exhibiting memory (e.g. of an external driver of the dynamical system)\n",
    "\n",
    "* This is usually realised by including a hidden state $\\mathbf{h}$\n",
    "\n",
    "$$\\mathbf h^{(t)} = f(\\mathbf h^{(t-1)}, \\mathbf x^{(t)}; \\mathbf \\theta),$$\n",
    "\n",
    "* We update the hidden state at each iteration, and the output of at each step is usually given as a function of the hidden state:\n",
    "\n",
    "$$\\mathbf{s}^{(t)} = g(\\mathbf{h}^{(t)}, \\mathbf{s}^{(t-1)}; \\theta)$$\n",
    "\n",
    "This will be the **general form of a RNN** that we will consider. \n",
    "\n",
    "* A RNN can develop self-sustained dynamics due to its recurrent connections, even in the absence of input. Indeed, it can be shown that, under fairly mild and general assumptions, that RNNs are **universal approximators of dynamical systems**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7aff5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* RNNs suffer from the **vanishing and exploding gradient problem**\n",
    "* When computing gradients through a long chain of RNN iterations, the gradients are scaled with the eigenvalues of the iteration operation\n",
    "* This can lead to the gradients vanishing or exploding \n",
    "* There are several ways to mitigate this problem\n",
    "* One particular simple one is **Reservoir Computing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0895653",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reservoir Computing \n",
    "\n",
    "* Reservoir computing is one implemenation of RNNs \n",
    "\n",
    "![rnn1](notebook-assets/reservoir.png)\n",
    "\n",
    "* They are ANNs with \n",
    "    * an input layer $W_{in}$\n",
    "    * a large hidden layer, the reservoir, that is usually a sparse random network $W$, with the (hidden) reservoir state vector $\\mathbf{r}(n)$\n",
    "    * an output layer $W_{out}$\n",
    "    \n",
    "* The key trick that reservoir computing does is that **only the output layer is trained** and the input layer and the reservoir are randomly initialized but constant \n",
    "\n",
    "* This has the advantage that the training can be done via linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ddf1e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "With $n$ as the discrete time index, $\\mathbf x(n) \\in \\mathbb{R}^{N_u}$ as the input signal, $\\mathbf r(n) \\in \\mathbb{R}^{N_x}$ are the reservoir activations, $\\mathbf W^{\\mathrm{in}} \\in \\mathbb{R}^{N_x \\times N_u}$ are the input-to-reservoir weights, $\\mathbf W \\in \\mathbb{R}^{N_x \\times N_x}$ are the reservoir-to-reservoir weights, $\\mathbf W^{\\mathrm{out}} \\in \\mathbb{R}^{N_y \\times N_x}$ are the reservoir-to-output weights, and $\\mathbf y(n) \\in \\mathbb{R}^{N_y}$ is the network output.\n",
    "\n",
    "The reservoir update equations follow as: \n",
    "$$\\begin{aligned}\n",
    "\\mathbf{r} (n) &= \\tanh\\left (\\mathbf W^{\\mathrm{in}}\\mathbf x (n) + \\mathbf W \\mathbf r (n-1)\\right ), \\\\\n",
    "\\mathbf y(n) &= \\mathbf{W}^{\\mathrm{out}}\\mathbf r(n).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- **The essential feature of a reservoir computer is that $\\mathbf W$ and $\\mathbf W^{\\mathrm{in}}$, which define the reservoir dynamics, are generated randomly and do not change, only $\\mathbf{W}^{(out)}$ is trained**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0afffd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications \n",
    "\n",
    "* Reservoir computing has been both applied successfully to prototypical chaotic systems, and climate phenomena\n",
    "* It is best suited for low-dimensional dynamical systems\n",
    "* See e.g:\n",
    "    1. [Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data (Pathak et al. 2017)](https://aip.scitation.org/doi/10.1063/1.5010300)\n",
    "    3. [Seasonal prediction of Indian summer monsoon onset with echo state networks (Mitsui & Boers 2021)](https://iopscience.iop.org/article/10.1088/1748-9326/ac0acb)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26b02e",
   "metadata": {},
   "source": [
    "## Applications \n",
    "\n",
    "* Reservoir computing used to predict the onset of the Indian Summer monsoon [(source)](https://iopscience.iop.org/article/10.1088/1748-9326/ac0acb/pdf)\n",
    "![Taka-Reservoir](notebook-assets/taka-reservoir.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0637df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Symbolic Methods (SINDy)\n",
    "\n",
    "* Another approach to estimate the dynamical system is to try to directly estimate it's equation from data, so that we really to reconstruct the symbolic expression of $f$ and not just numerically approximate the $f$ with a function approximator such as an ANN\n",
    "\n",
    "* **Symbolic Regression** \n",
    "\n",
    "    * But instead of already prescriping the functional form in a concrete way, can't we also let an algorithm find the functional form? \n",
    "    * Symbolic Regression tries to find the mathematical expression that best fits the $\\mathbf{X}$\n",
    "    * Applied to dynamical systems, it usually tries to find the mathematical expression for the right hand side $f$ of the system\n",
    "    * Therefore we often also need derivative data (e.g. computed with finite differences) \n",
    "    * Symbolic regression usually provides a dictionary of possible expressions (e.g. polynomials up to a certain degree, trigonemtric functions, etc ...) and than performs a regression to dermine the coefficents or parameters of these elementary functions\n",
    "    * But there are infintely many combinations of expressions: \n",
    "    \n",
    "    ![Symbolic Regression](notebook-assets/slice2.jpg)\n",
    "    \n",
    "    * Most natural laws and equations just involve a handful of terms, a candidate model should be complex enough to replicate the behaviour of the system but also \"simple\" (see Occam's razor)\n",
    "    * Therefore often some form of sparsity constraint is applied to the regression and one choses to only consider certain operations and experessions \n",
    "    * [AI Feynmann by Udrescu and Tegmark](https://arxiv.org/abs/1905.11481) attracted some attention: they do a symbolic regression with several different pre- and post-processing steps and apply it successfully to Feynmann's physics course books\n",
    "    \n",
    "\n",
    "* All of these methods have limitations when the complexity of the problem increases, data gets noisy and high-dimensional\n",
    "\n",
    "* One reason: There are just too many possible combinations of expressions to be considered \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade73cfd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projects\n",
    "\n",
    "Now it's your turn! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477b37b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Projects \n",
    "\n",
    "* For each of the methods we prepared recources for you to get started with them and work on a small project. \n",
    "* For each of the three main topics, we have a few suggestions of what you could do with them\n",
    "* You find everything at https://github.com/TUM-PIK-ESM/ML-DS-Workshop-23 \n",
    "\n",
    "* Best, you clone this repository to your local drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc24e43",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recources  \n",
    "\n",
    "* A Julia cheat-sheet \n",
    "* [Projects with Neural Differential Equations](projects/NeuralDifferentialEquations/NeuralDifferentialEquations.ipynb)\n",
    "* Reservoir Computing \n",
    "* SINdy"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
