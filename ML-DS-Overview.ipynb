{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c8841a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning for Dynamical Systems \n",
    "\n",
    "How can we learn dynamical systems from data? \n",
    "\n",
    "How can we use Machine Learning methods like Artificial Neural Networks for dynamical sytems? \n",
    "\n",
    "Can we incorporate prior knowledge into these methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cd621",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Methods for this Workshop \n",
    "\n",
    "* Neural Differential Equations \n",
    "* Reservoir Computing \n",
    "* Symbolic Methods (SINDy) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81ddf0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial Neural Networks: A quick summary\n",
    "\n",
    "Artificial neural networks (ANNs), in the form of multilayer perceptrons (MLPs) are:\n",
    "\n",
    "* Networks made up of a chain of several layers: $$f(\\mathbf x) = f^{(3)}(f^{(2)}(f^{(1)}(\\mathbf x; \\theta^{(1)}); \\theta^{(2)}); \\theta^{(3)}),$$ where each layers is a parametrized nonlinear transformation: $$\\sigma(\\mathbf z^{\\mathrm T} \\mathbf W + \\mathbf b),$$ where $\\theta=\\{\\mathbf{W},\\mathbf{b}\\}$ are the learnable parameters and $\\sigma$ is a nonlinear activation function, such as $\\tanh$\n",
    "* They are *universal function approximators* with many parameters $\\theta$ (in program code here often `p`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b101c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Given training data, we seek the best set of parameters by minimzing a loss function $L(\\theta)$, e.g. a mean square error, on that training data, by means of a gradient descent optimization, for which we need $$\\nabla_\\theta L(\\mathbf \\theta).$$ \n",
    "* The gradients are computed via *backpropagation*, i.e. chain rule of derivatives\n",
    "* ML libaries compute this via an automatic differenation system, that is able to systematically track all elementary operations performed\n",
    "* The parameters are then updated with some form of gradient descent \n",
    "$$\\theta_{new} = \\theta - \\eta \\nabla_\\theta L(\\mathbf \\theta)$$, \n",
    "with some learning rate $\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2828019",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Key Ingredients \n",
    "\n",
    "* **Training data**: usually pairs of input and output examples $(x,y)$ of the function we try to approximate\n",
    "* **A parametrized model**: e.g. a MLP \n",
    "* **A loss function**: e.g a least square error $L(\\theta)=\\sum \\left(\\text{MLP}(x;\\theta) - y\\right)^2$\n",
    "* The ability to take **gradients of the loss function**, to update the parameters of the model $\\theta_{new} = \\theta - \\eta \\nabla_\\theta L(\\mathbf \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec2676",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So how can we use these approaches for dynamical systems? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f8863",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Approximate a Dynamical System \n",
    "\n",
    "\n",
    "* Suppose we have a dynamical system $$\\frac{d\\mathbf{x}}{dt} = f(\\mathbf{x},t;\\theta)$$ that we observe some data from $\\mathbf{X}=\\{\\mathbf{x}(t_i)\\}$ for $t_i\\in[t_0;t_f]$, in the beginning we will restrict ourselves to evenly sampled observations $t_i = i\\cdot\\Delta t + t_0$\n",
    "\n",
    "* The correspoding discretized dynamical system is $$\\mathbf{x}_{n+1} = g(\\mathbf{x}_n, t_n; \\theta)$$ where $g$ is one iteration of some numerical DE solver\n",
    "\n",
    "* With **Neural Differential Equations** we replaced $f$ with an ANN and use it as a universal function approximator to learn the right hand side of the dynamical system from the observation $\\mathbf{X}$\n",
    " \n",
    "* With **Recurrent Neural Networks** (RNNs) like **Reservoir Computing**, we try to replace $g$ with ANN an learn it from data \n",
    "\n",
    "* When we want to recover the actual analytical expression of $f$, we have to rely on symbolic methods such as **Sparse Indentification of Dynamical Systems (SINDy)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fdd37",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prior Knowledge \n",
    "\n",
    "* Aside from MLPs, there are many different types of ANNs, e.g:![image-2.png](projects/NeuralDifferentialEquations/notebook-assets/typesofanns.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4939141",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Differential Equations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef470f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Differential Equations \n",
    "\n",
    "* With Neural Differential Equations (Neural DEs) we try to find a way to combine knowledge that we have of systems in form of their governing equations with data-driven approximators such as ANNs\n",
    "\n",
    "* In fact, there is an analogy between differential equations and ANNs:\n",
    "\n",
    "* A Residual Network (ResNet) block is defined by $$\\mathbf{h}_{t+1} = \\mathbf{h}_t + f(\\mathbf{h}_t;\\theta_t),$$ where $f$ can be any combination of other neural network layers with parameters $\\theta_t$\n",
    "\n",
    "* Through their short cut connection, ResNets learn a residual (hence the name). They proved to be an effective architecture for a wide variety of problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca1c42",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Compare that to the Euler solver to discretize and solve an ODE: \n",
    "$$\\begin{align}  \n",
    "\\frac{d\\mathbf{h}(t)}{dt} &= f(\\mathbf{h}(t),t;\\theta)\\\\\n",
    "\\mathbf{h}_{t+1} &= \\Delta t f(\\mathbf{h}_t, t;\\theta) + \\mathbf{h}_t\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "* Differential equations can be seen as a continuous time limit of ResNet ANNs\n",
    "* There are several paper that use to just solve ResNets with ODE solvers, but this is not our primary interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b94c5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Universal Differential Equations Framework\n",
    "\n",
    "* If we can treat differential equations and ANNs so similar, we just combine them directly:\n",
    "\n",
    "$$\\frac{du}{dt} = f(u,t,U_\\theta(u,t)),$$ \n",
    "where $U_{\\theta}$ is some data-driven function approximator (such as an ANN)\n",
    "  \n",
    "![image-2.png](projects/NeuralDifferentialEquations/notebook-assets/overview2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200e035",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We can integrate these Neural Differential Equations numerically, like any other differential equation we've seen before, resulting in a trajectory $\\hat{\\mathbf{u}}(\\mathbf{x},t;u_0, \\theta)$ for PDEs or $\\hat{\\mathbf{x}}(t;x_0, \\theta)$ for ODEs\n",
    "\n",
    "* `DiffEqFlux.jl`/`DiffEqSensitivity.jl` are by far the most comprehensive implementation of this approach (of all programming languages)\n",
    "* `torchdiffeq` offers some of the functionality for pyTorch\n",
    "* `diffrax` offers some of the functionality for JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4d124",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to train them? \n",
    "\n",
    "* We train them by minimizing a loss function $L(\\theta)$, e.g.: \n",
    "    * Given some example trajectories $\\mathbf{u}$ as training data, we can define a loss function $$L(\\theta)= \\sum_{i_t,\\mathbf{x}} ( \\mathbf{u}(\\mathbf{x},i_t) - \\hat{\\mathbf{u}}(\\mathbf{x},i_t;\\theta) )^2$$\n",
    "\n",
    "* For this approach to work we need the ability to take the derivatives of the trajectories\n",
    "* We can in fact do this by combining adjoint sensitivity analysis with automatic differentiation techniques (AD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a58eca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Adjoint Sensitivity Analysis \n",
    "\n",
    "* We can compute derivates $\\frac{\\partial\\hat{u}(\\mathbf{x},i_t;\\theta)}{\\partial\\theta}$ by the means of adjoint sensitivity analysis and AD\n",
    "* Adjoint sensitivity analysis solves an augmented version of the original DE system, appended with adjoint, e.g. $\\frac{d \\mathbf{a}}{d t} = - \\mathbf{a}^T(t) \\frac{\\partial f(\\mathbf{x}(t),t,\\theta)}{\\partial \\mathbf{x}(t)}$ that helps to compute the wanted derivatives \n",
    "* More details can be found in the [additional material of the Neural DE project](projects/NeuralDifferentialEquations/notebook-assets/ReverseModeSensitivityAnalysis.ipnyb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950232f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Are Gradients of Chaotic Systems Meaningful?\n",
    "\n",
    "* Nearby trajectories of chaotic systems diverge exponentially with their Lyapunov exponents, can we still compute them in meaningful ways?\n",
    "\n",
    "* Two possible ways:  \n",
    "    * Least square shadowing (for ergodic system and very long trajectories) \n",
    "    * \"Multiple shooting\": Start with short trajectories, slowly increase the length \n",
    "$$L(\\theta)= \\sum_{i_t}^{\\tau} \\sum_\\mathbf{x} ( \\mathbf{u}(\\mathbf{x},i_t) - \\hat{\\mathbf{u}}(\\mathbf{x},i_t;\\theta) )^2$$\n",
    "        * Start with $\\tau$ small, increase it during training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c1b0e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example Applications \n",
    "\n",
    "![CGLE](notebook-assets/cgle.png) \n",
    "\n",
    "[(source)](https://iopscience.iop.org/article/10.1088/1367-2630/abeb90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527bffd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reservoir Computing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f04c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Reservoir computing is a type of **Recurrent neural networks (RNNs)**,  a family of neural networks for processing sequential data.\n",
    "\n",
    "* Consider a sequence of the form\n",
    "\n",
    "$$\\mathbf s^{(t)} = f(\\mathbf s^{(t-1)}; \\mathbf \\theta),$$\n",
    "\n",
    "where $\\mathbf s^{(t)}$ is a vector describing the value of the sequence at the discrete index $t$, and $\\mathbf \\theta$ are some parameters of the function $f$.\n",
    "\n",
    "![rnn1](notebook-assets/rnn-1.png) [Source](https://www.deeplearningbook.org/)\n",
    "\n",
    "* We call such a sequence **recurrent** because the definition of $\\mathbf s$ at time $t$ refers back to the same definition at time $t-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add838c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* More generally, we can also consider the RNN exhibiting memory (e.g. of an external driver of the dynamical system)\n",
    "\n",
    "* This is usually realised by including a hidden state $\\mathbf{h}$\n",
    "\n",
    "$$\\mathbf h^{(t)} = f(\\mathbf h^{(t-1)}, \\mathbf x^{(t)}; \\mathbf \\theta),$$\n",
    "\n",
    "* We update the hidden state at each iteration, and the output of at each step is usually given as a function of the hidden state:\n",
    "\n",
    "$$\\mathbf{s}^{(t)} = g(\\mathbf{h}^{(t)}, \\mathbf{s}^{(t-1)}; \\theta)$$\n",
    "\n",
    "This will be the **general form of a RNN** that we will consider. \n",
    "\n",
    "* A RNN can develop self-sustained dynamics due to its recurrent connections, even in the absence of input. Indeed, it can be shown that, under fairly mild and general assumptions, that RNNs are **universal approximators of dynamical systems**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7aff5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* RNNs suffer from the **vanishing and exploding gradient problem**\n",
    "* When computing gradients through a long chain of RNN iterations, the gradients are scaled with the eigenvalues of the iteration operation\n",
    "* This can lead to the gradients vanishing or exploding \n",
    "* There are several ways to mitigate this problem\n",
    "* One particular simple one is **Reservoir Computing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047e9bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reservoir Computing \n",
    "\n",
    "<div><center>\n",
    "    <img src=\"notebook-assets/reservoir2.png\" width=\"500\"/></center>\n",
    "</div>\n",
    "\n",
    "* They are RNNs with \n",
    "    * an input layer $W_{in}$\n",
    "    * a large hidden layer, the reservoir, that is usually a sparse random network $W$, with the (hidden) reservoir state vector $\\mathbf{r}(n)$\n",
    "    * an output layer $W_{out}$\n",
    "    \n",
    "* The key trick that reservoir computing does is that **only the output layer is trained** and the input layer and the reservoir are randomly initialized but constant \n",
    "* This has the advantage that the training can be done via linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7dfc0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reservoir Ingredients\n",
    "<div><center>\n",
    "    <img src=\"notebook-assets/reservoir2.png\" width=\"300\"/></center>\n",
    "</div>\n",
    "\n",
    "* With $n$ as the discrete time index\n",
    "* $\\mathbf x(n) \\in \\mathbb{R}^{N_x}$ as the input signal\n",
    "* $\\mathbf r(n) \\in \\mathbb{R}^{N_r}$ are the reservoir activations\n",
    "* $\\mathbf W^{\\mathrm{in}} \\in \\mathbb{R}^{N_r \\times N_x}$ are the input-to-reservoir weights\n",
    "* $\\mathbf W \\in \\mathbb{R}^{N_r \\times N_r}$ are the reservoir-to-reservoir weights\n",
    "* $\\mathbf W^{\\mathrm{out}} \\in \\mathbb{R}^{N_y \\times N_r}$ are the reservoir-to-output weights \n",
    "* $\\mathbf y(n) \\in \\mathbb{R}^{N_y}$ is the network output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c40ce81",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reservoir Updates \n",
    "\n",
    "The reservoir update equations follow as: \n",
    "$$\\begin{aligned}\n",
    "\\mathbf{r} (n) &= \\tanh\\left (\\mathbf W^{\\mathrm{in}}\\mathbf x (n) + \\mathbf W \\mathbf r (n-1)\\right ), \\\\\n",
    "\\mathbf y(n) &= \\mathbf{W}^{\\mathrm{out}}\\mathbf r(n).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- **The essential feature of a reservoir computer is that $\\mathbf W$ and $\\mathbf W^{\\mathrm{in}}$, which define the reservoir dynamics, are generated randomly and do not change, only $\\mathbf{W}^{(out)}$ is trained**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0afffd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications \n",
    "\n",
    "* Reservoir computing has been both applied successfully to prototypical chaotic systems, and climate phenomena\n",
    "* It is best suited for low-dimensional dynamical systems\n",
    "* See e.g:\n",
    "    1. [Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data (Pathak et al. 2017)](https://aip.scitation.org/doi/10.1063/1.5010300)\n",
    "    3. [Seasonal prediction of Indian summer monsoon onset with echo state networks (Mitsui & Boers 2021)](https://iopscience.iop.org/article/10.1088/1748-9326/ac0acb)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99003062",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications \n",
    "\n",
    "* Reservoir computing used to predict the onset of the Indian Summer monsoon [(source)](https://iopscience.iop.org/article/10.1088/1748-9326/ac0acb/pdf)\n",
    "![Taka-Reservoir](notebook-assets/taka-reservoir.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668efa2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Symbolic Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0637df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Symbolic Methods \n",
    "\n",
    "* Another approach to estimate the dynamical system is to try to directly estimate its equation from data, so that we really try to reconstruct the symbolic expression of $f$ and not just numerically approximate the $f$ with a function approximator such as an ANN\n",
    "\n",
    "* This is the goal of **Symbolic Regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df099bc9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Symbolic Regression \n",
    "\n",
    "* Symbolic Regression tries to find the mathematical expression that best fits the $\\mathbf{X}$\n",
    "* Applied to dynamical systems, it usually tries to find the mathematical expression for the right hand side $f$ of the system\n",
    "* Therefore we often also need derivative data (e.g. computed with finite differences) \n",
    "* Symbolic regression usually provides a dictionary of possible expressions (e.g. polynomials up to a certain degree, trigonemtric functions, etc ...) and than performs a regression to dermine the coefficents or parameters of these elementary functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356cf8ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* But there are infintely many combinations of expressions: \n",
    "    \n",
    "    ![Symbolic Regression](notebook-assets/slice2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f217a5a5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sparse Symbolic Regression\n",
    "\n",
    "* Most natural laws and equations just involve a handful of terms, a candidate model should be complex enough to replicate the behaviour of the system but also \"simple\" (see Occam's razor)\n",
    "* Therefore often some form of sparsity constraint is applied to the regression and one choses to only consider certain operations and experessions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7fe2bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sparse Identification of Nonlinear Dynamics (SINDy)\n",
    "\n",
    "* SINDy ([Brunton et al. 2016](https://www.pnas.org/doi/10.1073/pnas.1517384113)) is one particular popular symbolic regression algorithm for dynamical systems \n",
    "\n",
    "* SINDy therefore attempts to fit a **sparse** model of the form,\n",
    "\n",
    "$$\\dot{x}_k = f_k(\\mathbf x) = \\Theta(\\mathbf x^T) \\mathbf\\xi_k,$$\n",
    "\n",
    "where $k = 1, 2, \\ldots, n$ denotes the component of $\\mathbf f$ being modelled, $\\Theta(\\mathbf x^T)$ is a vector of symbolic (generally nonlinear) basis functions of the elements of $\\mathbf x$, and $\\mathbf\\xi_k$ is a vector of sparse coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96f7aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The SINDy algorithm thus boils down to choosing an appropriate basis $\\Theta$ before solving $n$ independent sparse regression problems for the coefficients $\\Xi = [\\xi_1 \\xi_2 \\dots \\xi_n]$.\n",
    "<br>\n",
    "\n",
    "* When all is said and done, we are left with a model of the form\n",
    "\n",
    "$$\\mathbf{\\dot{x}} = \\mathbf{f}(\\mathbf{x}) \\approx \\Xi^T ( \\Theta(\\mathbf{x}^T)) ^T,$$\n",
    "\n",
    "which can be expanded out in terms of the symbolic basis functions $\\Theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9257bd72",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](notebook-assets/sindy.png)\n",
    "\n",
    "Figure: [Brunton et al. 2016](https://www.pnas.org/doi/10.1073/pnas.1517384113)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc3393c",
   "metadata": {},
   "source": [
    "### Limitations & Possibilities  \n",
    "\n",
    "* All of these methods have limitations when the complexity of the problem increases, data gets noisy and high-dimensional\n",
    "* One reason: There are just too many possible combinations of expressions to be considered \n",
    "* There is one thing we can try though!\n",
    "    * We can first train an ML model like Neural DEs and then use SINDy on this model\n",
    "    * This elimnates some of the shortcomings \n",
    "    * With the Neural DE we can generate as much training data for SINDy as we want\n",
    "    * You can try this as a project! We prepared a lot of the needed tools for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477b37b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Projects \n",
    "\n",
    "* For each of the methods we prepared recources for you to get started with them and work on a small project. \n",
    "* For each of the three main topics, we have a few suggestions of what you could do with them\n",
    "* You find everything at https://github.com/TUM-PIK-ESM/ML-DS-Workshop-23 \n",
    "\n",
    "* Best, you clone this repository to your local drive\n",
    "\n",
    "* All of the code that we provide is written in Julia. So in the next session we will have quick introduction to Julia and how to numerically solve dynamical systems with it "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc24e43",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recources  \n",
    "\n",
    "* [A Julia cheat-sheet](https://cheatsheet.juliadocs.org)\n",
    "* [General Project Information](project-info.pdf)\n",
    "* [Project Method I: Neural Differential Equations](projects/NeuralDifferentialEquations/NeuralDifferentialEquations.ipynb)\n",
    "* [Project Method II: Reservoir Computing](projects/ReservoirComputing/reservoir_computing.ipynb)\n",
    "* [Project Method III: Symbolic Regression / SINDy](projects/SINDy/SINDy.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
